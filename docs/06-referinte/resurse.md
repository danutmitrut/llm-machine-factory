# Resurse suplimentare

Lectură și instrumente pentru a aprofunda.

---

## Fundamentele teoretice

### Chain of Thought Prompting
- **Paper:** "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" (Wei et al., 2022)
- **Idee cheie:** forțarea LLM-urilor să-și arate pașii de raționament îmbunătățește semnificativ acuratețea

### Tree of Thought
- **Paper:** "Tree of Thoughts: Deliberate Problem Solving with Large Language Models" (Yao et al., 2023)
- **Idee cheie:** explorarea mai multor căi de raționament în paralel și selectarea celei mai promițătoare

### Self-Consistency
- **Paper:** "Self-Consistency Improves Chain of Thought Reasoning in Language Models" (Wang et al., 2022)
- **Idee cheie:** generarea mai multor răspunsuri și selectarea celui mai consistent

---

## Instrumente complementare

### Pentru verificare factuală
- **Perplexity AI** - căutare web cu citări
- **Consensus** - căutare în literatura științifică
- **Elicit** - analiză de research papers

### Pentru construcție de prompturi
- **PromptPerfect** - optimizare automată de prompturi
- **Promptbase** - marketplace de prompturi
- **FlowGPT** - comunitate de prompturi

### Pentru testare și iterație
- **LangSmith** - debugging pentru aplicații LLM
- **Weights & Biases** - tracking experimente
- **Promptfoo** - testing automatizat de prompturi

---

## Comunități și learning

### Unde să înveți mai mult
- **r/PromptEngineering** - comunitate Reddit
- **Latent Space** - podcast despre AI
- **AI Snake Oil** - analiză critică a claims-urilor AI

### Cursuri recomandate
- **DeepLearning.AI** - Prompt Engineering pentru dezvoltatori
- **Anthropic's Prompt Engineering Guide** - ghid oficial Claude
- **OpenAI Cookbook** - exemple și best practices

---

## Lecturi conexe

### Despre limitările LLM-urilor
- "On the Dangers of Stochastic Parrots" (Bender et al.)
- "Language Models are Few-Shot Learners" (Brown et al.)

### Despre meta-cogniție în AI
- "Reflexion: Language Agents with Verbal Reinforcement Learning"
- "Constitutional AI: Harmlessness from AI Feedback"

### Despre halucinații
- "Survey of Hallucination in Natural Language Generation"
- "TruthfulQA: Measuring How Models Mimic Human Falsehoods"

---

## Tools pentru implementare

### APIs
- **OpenAI API** - GPT-4, GPT-3.5
- **Anthropic API** - Claude 3 (Opus, Sonnet, Haiku)
- **Google AI** - Gemini
- **Mistral AI** - Mistral, Mixtral

### Frameworks
- **LangChain** - orchestrare LLM
- **LlamaIndex** - RAG și indexare
- **Semantic Kernel** - Microsoft's AI orchestration

### No-code
- **Make.com** - automatizări
- **Zapier** - integrări
- **n8n** - workflow automation (self-hosted)

---

## Actualizări și suport

### Acest pachet
- **GitHub:** [link la repo]
- **Actualizări:** verifică periodic pentru versiuni noi
- **Feedback:** deschide un issue pentru sugestii

### Contact autor
- Dan Mitrut
- [Contact info]

---

## Changelog

### Ianuarie 2025
- Release inițial V1.0
- Include: AIM, Arhitect soluții V2.3, Auditor veridicitate V1.0, Arhitect meta-cognitive V1.0
- Documentație completă în română

### Planned
- versiune în engleză
- mașini adiționale specializate
- studii de caz extinse
- video tutorials

---

## Licență

Acest material este oferit pentru uz personal și educațional.
Pentru utilizare comercială, contactează autorul.
